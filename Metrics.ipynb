{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e663c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cf3cb",
   "metadata": {},
   "source": [
    "# Merge/Push problem solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def intersection_num(list1, list2):\n",
    "    return len(list(set(list1).intersection(list2)))\n",
    "def union_num(list1, list2):\n",
    "    return (len(set(list1)) + len(set(list2))) - intersection_num(list1, list2)\n",
    "\n",
    "def intersection_list(list1, list2):\n",
    "    return list(set(list1).intersection(list2))\n",
    "def union_list(list1, list2):\n",
    "    return list(set(list(set(list1)) + list(set(list2))))\n",
    "\n",
    "def p_n(nouns1, nouns2):\n",
    "    intersection = intersection_num(nouns1, nouns2)\n",
    "    union = union_num(nouns1, nouns2)\n",
    "    if union==0:\n",
    "        return 0\n",
    "    return abs(float(intersection) / union)\n",
    "\n",
    "def p_a_v(a1, v1, a2, v2):\n",
    "    union_a_v_1 = union_list(a1, v1)\n",
    "    union_a_v_2 = union_list(a2, v2)\n",
    "    \n",
    "    intersection_top = abs(intersection_num(union_a_v_1, union_a_v_2)) + 1\n",
    "    union_bot = abs(union_num(union_a_v_1, union_a_v_2)) + 2\n",
    "    return float(intersection_top) / union_bot\n",
    "\n",
    "def p_x(p_n, p_a_v):\n",
    "    return p_n * p_a_v\n",
    "\n",
    "\n",
    "def tokenize_food(st):\n",
    "    wordsList = nltk.word_tokenize(st)\n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    #  Using a Tagger. Which is part-of-speech tagger\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    \n",
    "    NOUN_TAGS = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "    ADJECTIVE_TAGS = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "    VERB_TAGS = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    CARDINAL_TAGS = [\"CD\"]\n",
    "    ADVERB_TAGS = [\"RB\", \"RBR\", \"RBS\"]\n",
    "    PRONOUN_TAGS = [\"PRP\", \"PRP$\"]\n",
    "    \n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    verbs = []\n",
    "    cardinal_numbers = []\n",
    "    adverbs = []\n",
    "    pronouns = []\n",
    "    other = []\n",
    "    for word, tag in tagged:\n",
    "        if tag in NOUN_TAGS:\n",
    "            nouns.append(word)\n",
    "            continue\n",
    "        if tag in ADJECTIVE_TAGS:\n",
    "            adjectives.append(word)\n",
    "            continue\n",
    "        if tag in VERB_TAGS:\n",
    "            verbs.append(word)\n",
    "            continue\n",
    "        if tag in CARDINAL_TAGS:\n",
    "            cardinal_numbers.append(word)\n",
    "            continue\n",
    "        if tag in ADVERB_TAGS:\n",
    "            adverbs.append(word)\n",
    "            continue\n",
    "        if tag in PRONOUN_TAGS:\n",
    "            pronouns.append(word)\n",
    "            continue\n",
    "        other.append(word)\n",
    "    \n",
    "    nouns_lemmatized = []\n",
    "    for noun in nouns:\n",
    "        nouns_lemmatized.append(lemmatizer.lemmatize(noun, \"n\"))\n",
    "    nouns_lemmatized = list(set(nouns_lemmatized))\n",
    "    \n",
    "    adjectives_lemmatized = []\n",
    "    for adjective in adjectives:\n",
    "        adjectives_lemmatized.append(lemmatizer.lemmatize(adjective, \"a\"))\n",
    "    adjectives_lemmatized = list(set(adjectives_lemmatized))\n",
    "    \n",
    "    verbs_lemmatized = []\n",
    "    for verb in verbs:\n",
    "        verbs_lemmatized.append(lemmatizer.lemmatize(verb, \"v\"))\n",
    "    verbs_lemmatized = list(set(verbs_lemmatized))\n",
    "    \n",
    "    adverbs_lemmatized = []\n",
    "    for adverb in adverbs:\n",
    "        adverbs_lemmatized.append(lemmatizer.lemmatize(adverb, \"r\"))\n",
    "    adverbs_lemmatized = list(set(adverbs_lemmatized))\n",
    "    \n",
    "    return nouns_lemmatized, adjectives_lemmatized, verbs_lemmatized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lexical_similarity(s1, s2):\n",
    "    nouns_1, adjectives_1, verbs_1 = tokenize_food(s1)\n",
    "    nouns_2, adjectives_2, verbs_2 = tokenize_food(s2)\n",
    "    return p_x(p_n(nouns_1, nouns_2), p_a_v(adjectives_1, verbs_1, adjectives_2, verbs_2))\n",
    "\n",
    "\n",
    "def sentences_encoding(sentences):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    return sentence_embeddings\n",
    "    \n",
    "def my_clustering(sentence_embeddings, eps=0.01):\n",
    "    clustering = DBSCAN(eps=eps, min_samples=2, metric=\"cosine\", n_jobs=-1).fit(sentence_embeddings)\n",
    "\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    lbl_set = set(labels)\n",
    "    #print(labels[:20], len(lbl_set))\n",
    "    return labels, len(lbl_set)\n",
    "\n",
    "REGEX_ENGLISH = '^[a-zA-Z]+[a-zA-Z\\d\\W]+[a-zA-Z]+$'\n",
    "\n",
    "def return_english_only(l):\n",
    "    p = re.compile(REGEX_ENGLISH)\n",
    "    l2 = [ s for s in l if p.match(s) ]\n",
    "    return l2 \n",
    "\n",
    "def remove_rows_with_non_english_foodname(df, col_name):\n",
    "    \n",
    "    eng = df[col_name].str.contains(REGEX_ENGLISH)\n",
    "    df1 = df[eng]\n",
    "    return df1\n",
    "\n",
    "\n",
    "    \n",
    "def pipeline(sentences, clust_eps=0.01, sim_measure=0.7, ignore_errors=False, skip_clustering=False, debug=False):\n",
    "    print(\"Filtering for english text only\")\n",
    "    sentences = return_english_only(sentences)\n",
    "    if not skip_clustering:\n",
    "        print(\"Producing embeddings..\")\n",
    "        sentence_embeddings = sentences_encoding(sentences)\n",
    "        print(\"Embeddings Done!\")\n",
    "        print(\"Doing Clustering..\")\n",
    "    \n",
    "        labels, len_lbl_set= my_clustering(sentence_embeddings, eps=clust_eps)\n",
    "\n",
    "        desc_clusters = {}\n",
    "        for sentence, cluster in zip(sentences, labels):\n",
    "            desc_clusters[cluster] = desc_clusters.get(cluster, [])\n",
    "            desc_clusters[cluster].append(sentence)\n",
    "\n",
    "        print(f\"Clustering done, {len_lbl_set} clusters found\")\n",
    "        \n",
    "        desc_clusters_matches = desc_clusters\n",
    "        non_clustered = deepcopy(desc_clusters_matches[-1])\n",
    "        desc_clusters_matches[-1]=[]\n",
    "    else: \n",
    "        desc_clusters_matches = {0:sentences}\n",
    "        \n",
    "    print(f\"Searching for lexical matches..\")\n",
    "    \n",
    "    #print(desc_clusters_matches[0])\n",
    "\n",
    "    matches = []\n",
    "    for number, items in tqdm(desc_clusters_matches.items()):\n",
    "        print(f\"Processing Cluster of {len(items)} items\")\n",
    "        i_prev = 0 \n",
    "        for i in range(50, len(items), 50):\n",
    "            items_slice = items[i_prev:i]\n",
    "            i_prev = i\n",
    "            for item1 in items_slice:\n",
    "                for item2 in items_slice:\n",
    "                    if item1 != item2 and lexical_similarity(item1, item2) > sim_measure and (item1, item2) not in matches and (item2, item1) not in matches:\n",
    "                        matches.append((item1, item2))\n",
    "    \n",
    "    print(f\"{len(matches)} matches found\")\n",
    "    if not skip_clustering:\n",
    "        if debug:\n",
    "            return desc_clusters, sentence_embeddings, labels, matches\n",
    "        return sentence_embeddings, labels, matches\n",
    "    else:\n",
    "        return matches\n",
    "        \n",
    "\n",
    "\n",
    "# df1 and df2 are dataframes where first column is the name of the dish\n",
    "# All other columns can only be ingredients\n",
    "# If names_only is True then we use different thresholds for shorter matches\n",
    "# If hard_maches is True, then we use stricter thresholds\n",
    "def find_matches(df1, df2, names_only=False, hard_matches=False, check_duplicates=False, sim_measure=None, clust_eps=None, debug=False):\n",
    "    df1.rename(columns={df1.columns[0]: \"FoodName\" }, inplace = True)\n",
    "    df2.rename(columns={df2.columns[0]: \"FoodName\" }, inplace = True)\n",
    "    \n",
    "    sentences_1 = list(df1.sum(axis=1))\n",
    "    sentences_2 = list(df2.sum(axis=1))\n",
    "\n",
    "    if names_only:\n",
    "        if hard_matches:\n",
    "            similarity_measure = 0.5 \n",
    "            clustering_epsilon = 0.05\n",
    "        else:\n",
    "            similarity_measure = 0.4\n",
    "            clustering_epsilon = 0.05\n",
    "    else:\n",
    "        if hard_matches:\n",
    "            similarity_measure = 0.8 \n",
    "            clustering_epsilon = 0.05\n",
    "        else:\n",
    "            similarity_measure = 0.7 \n",
    "            clustering_epsilon = 0.05\n",
    "    \n",
    "    if sim_measure is not None:\n",
    "        similarity_measure = sim_measure\n",
    "    if clust_eps is not None:\n",
    "        clustering_epsilon = clust_eps\n",
    "    \n",
    "    if debug:\n",
    "        desc_clusters, sentence_embeddings, labels, matches = \\\n",
    "        pipeline(set(sentences_1+sentences_2), clust_eps=clustering_epsilon, sim_measure=similarity_measure, debug=debug)\n",
    "    else:\n",
    "        sentence_embeddings, labels, matches = \\\n",
    "        pipeline(set(sentences_1+sentences_2), clust_eps=clustering_epsilon, sim_measure=similarity_measure, debug=debug)\n",
    "    name_matches = list(set(matches))\n",
    "    matches = []\n",
    "    for name_match in name_matches:\n",
    "        n1 = name_match[0]\n",
    "        n2 = name_match[1]\n",
    "        \n",
    "        match_1_1 = df1.loc[df1[\"FoodName\"] == str(n1)]\n",
    "        match_1_2 = df1.loc[df1[\"FoodName\"] == str(n2)]\n",
    "        \n",
    "        match_2_1 = df2.loc[df2[\"FoodName\"] == str(n1)]\n",
    "        match_2_2 = df2.loc[df2[\"FoodName\"] == str(n2)]\n",
    "        \n",
    "        match = {\"df1_name_1\":match_1_1, \"df1_name_2\":match_1_2,\n",
    "                                        \"df2_name_1\":match_2_1,\"df2_name_2\":match_2_2}\n",
    "\n",
    "        matches.append(match)\n",
    "    \n",
    "    if check_duplicates:\n",
    "        dupe_1 = list(set(duplicates(sentences_1)))\n",
    "        dupe_2 = list(set(duplicates(sentences_2)))\n",
    "\n",
    "        for d1 in dupe_1:        \n",
    "            match_1_1 = df1.loc[df1[\"FoodName\"] == str(d1)]\n",
    "            match_1_2 = df1.loc[df1[\"FoodName\"] == str(d1)]\n",
    "\n",
    "            match_2_1 = df2.loc[df2[\"FoodName\"] == str(\"asdasdqweqasbdashjdasd\")]\n",
    "            match_2_2 = df2.loc[df2[\"FoodName\"] == str(\"asdasdqweqasbdashjdasd\")]\n",
    "\n",
    "            match = {\"df1_name_1\":match_1_1, \"df1_name_2\":match_1_2,\n",
    "                                            \"df2_name_1\":match_2_1,\"df2_name_2\":match_2_2}\n",
    "\n",
    "            matches.append(match)\n",
    "\n",
    "        for d2 in dupe_2:        \n",
    "            match_1_1 = df1.loc[df1[\"FoodName\"] == str(\"asdasdqweqasbdashjdasd\")]\n",
    "            match_1_2 = df1.loc[df1[\"FoodName\"] == str(\"asdasdqweqasbdashjdasd\")]\n",
    "\n",
    "            match_2_1 = df2.loc[df2[\"FoodName\"] == d2]\n",
    "            match_2_2 = df2.loc[df2[\"FoodName\"] == d2]\n",
    "\n",
    "            match = {\"df1_name_1\":match_1_1, \"df1_name_2\":match_1_2,\n",
    "                                            \"df2_name_1\":match_2_1,\"df2_name_2\":match_2_2}\n",
    "\n",
    "            matches.append(match)\n",
    "        \n",
    "        dupe_1_2 = duplicates(list(set(sentences_1)) + list(set(sentences_2)))\n",
    "        \n",
    "        for d12 in dupe_1_2:        \n",
    "            match_1_1 = df1.loc[df1[\"FoodName\"] == d12]\n",
    "            match_1_2 = df1.loc[df1[\"FoodName\"] == str(\"asdasdqweqasbdashjdasd\")]\n",
    "\n",
    "            match_2_1 = df2.loc[df2[\"FoodName\"] == str(\"asdasdqweqasbdashjdasd\")]\n",
    "            match_2_2 = df2.loc[df2[\"FoodName\"] == d12]\n",
    "\n",
    "            match = {\"df1_name_1\":match_1_1, \"df1_name_2\":match_1_2,\n",
    "                                            \"df2_name_1\":match_2_1,\"df2_name_2\":match_2_2}\n",
    "\n",
    "            matches.append(match)\n",
    "    if debug:\n",
    "        return desc_clusters, matches\n",
    "    return matches\n",
    "\n",
    "def match_types(matches):\n",
    "    df1_self_matches = []\n",
    "    df2_self_matches = []\n",
    "    df1_to_df2 = []\n",
    "    for row in matches:\n",
    "        if (not (row[\"df1_name_1\"].empty)) and (not (row[\"df1_name_2\"].empty)):\n",
    "            df1_self_matches.append((row[\"df1_name_1\"], row[\"df1_name_2\"]))\n",
    "        \n",
    "        if (not (row[\"df2_name_1\"].empty)) and (not (row[\"df2_name_2\"].empty)):\n",
    "            df2_self_matches.append((row[\"df2_name_1\"], row[\"df2_name_2\"]))\n",
    "            \n",
    "        if (not (row[\"df1_name_1\"].empty)) and (not (row[\"df2_name_2\"].empty)):\n",
    "            df1_to_df2.append((row[\"df1_name_1\"], row[\"df2_name_2\"]))\n",
    "        \n",
    "        if (not (row[\"df1_name_2\"].empty)) and (not (row[\"df2_name_1\"].empty)):\n",
    "            df1_to_df2.append((row[\"df1_name_2\"], row[\"df2_name_1\"]))\n",
    "            \n",
    "    return df1_self_matches, df2_self_matches, df1_to_df2\n",
    "\n",
    "def show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2):\n",
    "    print(\"Input matches to itself:\", len(df1_self_matches))\n",
    "    print(\"Input to reference matches:\", len(df1_to_df2))\n",
    "\n",
    "\n",
    "def print_dict_nicely(d):\n",
    "    for k, v in d.items():\n",
    "        print(str(k).ljust(15), str(v).ljust(15))\n",
    "\n",
    "def duplicates(liststring):\n",
    "    seen = set()\n",
    "    dupes = [x for x in liststring if x in seen or seen.add(x)]\n",
    "    return dupes\n",
    "\n",
    "def match_column_names(col1, col2, sim_measure=0.4): \n",
    "    col_matches = []\n",
    "    for col_name in col1:\n",
    "        for col2_name in col2:\n",
    "            if col_name == col2_name or lexical_similarity(col_name, col2_name) > sim_measure:\n",
    "                col_matches.append((col_name, col2_name))\n",
    "    return col_matches\n",
    "\n",
    "def compare_numerical_columns_for_match(df1_match, df2_match, matching_columns):\n",
    "    res = {}\n",
    "    for column1, column2  in matching_columns:\n",
    "        try:\n",
    "            one = df1_match[column1][0]\n",
    "            two = df2_match[column2][0]\n",
    "            if one is None or two is None:\n",
    "                res[column2] = None\n",
    "                continue\n",
    "        except Exception:\n",
    "            res[column2] = None\n",
    "            continue\n",
    "        res[column2] = df1_match[column1][0] - df2_match[column2][0]\n",
    "    return res\n",
    "\n",
    "def compare_matches(matches, matching_columns):\n",
    "    res = []\n",
    "    for match in matches:\n",
    "        res.append(compare_numerical_columns_for_match(match[0], match[1], matching_columns))\n",
    "    return res\n",
    "    \n",
    "\n",
    "def match_clear_name_dirty(clear_name, dirty_names, clear_func):\n",
    "    for dn in dirty_names:\n",
    "        if clear_func(dn) == clear_name:\n",
    "            return dn\n",
    "    \n",
    "\n",
    "def match_dirty(clear_matches, dirty_names_1, dirty_names_2, clear_func_1, clear_func_2):\n",
    "    dirty_matches = []\n",
    "    for name1, name2 in clear_matches:\n",
    "        dirty_name1 = match_clear_name_dirty(name1, dirty_names_1, clear_func_1)\n",
    "        dirty_name2 = match_clear_name_dirty(name2, dirty_names_2, clear_func_2)\n",
    "        dirty_matches.append((dirty_name1, dirty_name2))\n",
    "    return dirty_matches\n",
    "\n",
    "\n",
    "def get_orig_row(name, orig_df, col_name):\n",
    "    return orig_df[orig_df[col_name] == name].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "def bad_matches_to_good_matches(matches, df1, df2, col_name_1, col_name_2):\n",
    "    better_matches = []\n",
    "    if df2 is None:\n",
    "        for matchl, matchr in tqdm(matches):\n",
    "            bl = get_orig_row(matchl.iloc[0,0], df1, col_name_1)\n",
    "            br = get_orig_row(matchr.iloc[0,0], df1, col_name_1)\n",
    "            better_matches.append((bl, br))\n",
    "    else: \n",
    "        for matchl, matchr in tqdm(matches):\n",
    "            bl = get_orig_row(matchl.iloc[0,0], df1, col_name_1)\n",
    "            br = get_orig_row(matchr.iloc[0,0], df2, col_name_2)\n",
    "            better_matches.append((bl, br))\n",
    "    \n",
    "    return better_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c4bb2",
   "metadata": {},
   "source": [
    "# Image quality helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure \n",
    "from skimage import io\n",
    "import imquality.brisque as brisque\n",
    "import PIL.Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def measure_over_images(image_dir):\n",
    "    average_entropy = 0\n",
    "    average_entropy_per_class = {}\n",
    "\n",
    "    image_sizes = {}\n",
    "\n",
    "    average_quality = 0\n",
    "    quality_per_class = {}\n",
    "    pictures_with_negative_quality = {}\n",
    "\n",
    "    average_sharpness = 0\n",
    "    sharpness_per_class = {}\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for dd in tqdm(listdir(image_dir)):\n",
    "        for f in tqdm(listdir(image_dir+dd)):\n",
    "            try:\n",
    "                i+=1\n",
    "                if i % 500 == 0:\n",
    "                    print(\"Entropy: \", average_entropy)\n",
    "                    print(\"Quality: \", average_quality)\n",
    "                    print(\"Negative quality: \", len(pictures_with_negative_quality))\n",
    "\n",
    "                img_path = image_dir + dd + \"/\"+ f\n",
    "\n",
    "                imgpl = PIL.Image.open(img_path)\n",
    "                img = np.array(imgpl)\n",
    "                \n",
    "                \n",
    "                entropy = skimage.measure.shannon_entropy(img)\n",
    "                average_entropy_per_class[f] = average_entropy_per_class.get(f, [])\n",
    "                average_entropy_per_class[f].append(entropy)\n",
    "                average_entropy += entropy\n",
    "\n",
    "                \n",
    "                quality = brisque.score(imgpl)\n",
    "                quality_per_class[f] = quality_per_class.get(f, [])\n",
    "                quality_per_class[f].append(quality)\n",
    "                average_quality+=quality\n",
    "\n",
    "                if quality<0:\n",
    "                    pictures_with_negative_quality[img_path] = quality\n",
    "\n",
    "                array = np.asarray(imgpl, dtype=np.int32)\n",
    "\n",
    "                image_sizes[str(array.shape)] = image_sizes.get(str(array.shape), 0)\n",
    "                image_sizes[str(array.shape)]+=1\n",
    "            except Exception as e:\n",
    "                print(\"Error Occured:\", str(e))\n",
    "                continue\n",
    "\n",
    "        print(\"Entropy: \", average_entropy)\n",
    "        print(\"Quality: \", average_quality)\n",
    "        print(\"Negative quality: \", len(pictures_with_negative_quality))\n",
    "\n",
    "\n",
    "    average_entropy =  average_entropy/len(records)\n",
    "    average_quality = average_quality/len(records)\n",
    "\n",
    "\n",
    "    print(\"Entropy: \", average_entropy)\n",
    "    print(\"Quality: \", average_quality)\n",
    "\n",
    "    return average_entropy, average_entropy_per_class, image_sizes, \\\n",
    "            average_quality, quality_per_class,pictures_with_negative_quality, \\\n",
    "            average_sharpness, sharpness_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915114e5",
   "metadata": {},
   "source": [
    "# Calculate Row Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_rows_with_nan(dataframe):\n",
    "    return sum([True for idx,row in dataframe.iterrows() if any(row.isnull())])\n",
    "def get_cells_with_nan(dataframe):\n",
    "    return sum(df_cleaner.isnull().values.ravel())\n",
    "\n",
    "def get_column_nan_contribution(dataframe):\n",
    "    columns_with_nan = {}\n",
    "    for idx,row in dataframe.iterrows():\n",
    "        for i, r in enumerate(row):\n",
    "            if pd.isnull(r):\n",
    "                columns_with_nan[dataframe.columns[i]] = columns_with_nan.get(dataframe.columns[i], 0) + 1\n",
    "    return {k: v for k, v in sorted(columns_with_nan.items(), key=lambda item: item[1])} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f10863",
   "metadata": {},
   "source": [
    "# Time helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af01859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tz_from_dataframe(df_in):\n",
    "    df = df_in.copy()\n",
    "    col_times = [ col for col in df.columns if any([isinstance(x, pd.Timestamp) for x in df[col]])]\n",
    "    for col in col_times:\n",
    "        df[col] = pd.to_datetime(\n",
    "            df[col], infer_datetime_format=True) \n",
    "        df[col] = df[col].dt.tz_localize(None) \n",
    "    return df\n",
    "\n",
    "def clean_dates(df_in, column_name):\n",
    "    df_in = df_in.copy(deep=True)\n",
    "    df_in[column_name] = pd.to_datetime(df_in[column_name], errors='coerce')\n",
    "\n",
    "    df_in[column_name].dt.tz_localize(None)\n",
    "\n",
    "    df_in[column_name].apply(lambda x: x.replace(tzinfo=None))\n",
    "\n",
    "\n",
    "\n",
    "    now = [pd.Timestamp(str(datetime.now().replace(tzinfo=None))) for _ in range(0, len(df_in))]\n",
    "    df_in[\"now\"] = now\n",
    "    df_in[\"now\"] = pd.to_datetime(df_in[\"now\"]) \n",
    "    df_in[\"now\"].dt.tz_localize(None)\n",
    "    df_in[\"now\"].apply(lambda x: x.replace(tzinfo=None))\n",
    "\n",
    "    df_in = remove_tz_from_dataframe(df_in)\n",
    "\n",
    "    df_in[column_name] = df_in[\"now\"] - df_in[column_name]\n",
    "    df_in[column_name] = df_in[column_name].dt.total_seconds()\n",
    "\n",
    "    year_in_seconds = 365*24*60*60\n",
    "\n",
    "    df_in[column_name]  = df_in[column_name] / year_in_seconds\n",
    "    df_in = df_in.drop(\"now\", axis=1)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14614ea0",
   "metadata": {},
   "source": [
    "# Load OpenFoodFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84235f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfoodfacts = pd.read_csv('en.openfoodfacts.org.products.tsv', sep='\\t')\n",
    "openfoodfacts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfoodfacts_cleaner = openfoodfacts.drop([\"url\", \"code\", \"created_t\", \"created_datetime\", \"last_modified_t\"], axis=1)\n",
    "openfoodfacts_minimal = openfoodfacts_cleaner.drop([\"packaging\", \"packaging_tags\", \"brands_tags\"], axis=1)\n",
    "openfoodfacts_minimal = openfoodfacts_minimal.dropna(subset=['product_name'])\n",
    "openfoodfacts_minimal = remove_rows_with_non_english_foodname(openfoodfacts_minimal, 'product_name')\n",
    "\n",
    "openfoodfacts_cleaner_dates = clean_dates(openfoodfacts_minimal, \"last_modified_datetime\")\n",
    "openfoodfacts_cleaner_dates = openfoodfacts_cleaner_dates.astype({'product_name':'string'})\n",
    "openfoodfacts_minimal.head()\n",
    "\n",
    "openfoodfacts_no_nan = openfoodfacts_minimal.copy(deep=True)\n",
    "openfoodfacts_no_nan.dropna(how='all', inplace=True)\n",
    "openfoodfacts_no_nan.dropna(thresh=len(openfoodfacts_no_nan.columns)-7, inplace=True)\n",
    "\n",
    "openfoodfacts_cleaner_dates[\"food_with_ingredients\"] = openfoodfacts_cleaner_dates[[\"product_name\", \"ingredients_text\"]].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193317ec",
   "metadata": {},
   "source": [
    "# Load Nutritional Values for Common foods and produce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "NVCF = pd.read_csv('nutrition.csv')\n",
    "\n",
    "number_regex = re.compile(r\"[0-9]+[.0-9]*\")\n",
    "\n",
    "\n",
    "\n",
    "def determine_col_type(df, col_name, by_name=False):\n",
    "    if by_name:\n",
    "        df[col_name].apply(to_grams)\n",
    "        if \"mcg\" in col_name:\n",
    "            return \"mcg\"\n",
    "        elif \"mg\" in col_name:\n",
    "            return \"mg\"\n",
    "        elif \"g\" in col_name:\n",
    "            return \"g\"\n",
    "        \n",
    "    mg_n = df[col_name].astype(str).str.count(\"mg\").sum()\n",
    "    mcg_n = df[col_name].astype(str).str.count(\"mcg\").sum()\n",
    "    g_n = df[col_name].astype(str).str.count(\"g\").sum() - mg_n - mcg_n\n",
    "    \n",
    "    df[col_name].apply(to_grams)\n",
    "    if max(g_n, mg_n, mcg_n) == g_n:\n",
    "        return \"g\"\n",
    "    if max(g_n, mg_n, mcg_n) == mg_n:\n",
    "        return \"mg\"\n",
    "    if max(g_n, mg_n, mcg_n) == mcg_n:\n",
    "        return \"mcg\"\n",
    "    return \"other\"\n",
    "    \n",
    "\n",
    "\n",
    "def get_column_types(df, columns):\n",
    "    f = {}\n",
    "    for col in columns:\n",
    "        typ = determine_col_type(df, col)\n",
    "        l = f.get(typ, [])\n",
    "        l.append(col)\n",
    "        f[typ] = l \n",
    "    return f\n",
    "\n",
    "def to_grams(x):\n",
    "    try:\n",
    "        x = str(x)\n",
    "        num = re.findall(number_regex, x)\n",
    "        num = float(num[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "    if \"mcg\" in x:\n",
    "        num = num / 1000000\n",
    "    elif \"mg\" in x:\n",
    "        num = num / 1000\n",
    "    return num\n",
    "\n",
    "column_list_NVCF = [\"serving_size\",\"calories\",\"total_fat\",\"saturated_fat\",\"cholesterol\",\n",
    "               \"sodium\",\"choline\",\"folate\",\"folic_acid\",\"niacin\",\"pantothenic_acid\",\n",
    "               \"riboflavin\",\"thiamin\",\"vitamin_a\",\"vitamin_a_rae\",\"carotene_alpha\",\n",
    "               \"carotene_beta\",\"cryptoxanthin_beta\",\"lutein_zeaxanthin\",\"lucopene\",\n",
    "               \"vitamin_b12\",\"vitamin_b6\",\"vitamin_c\",\"vitamin_d\",\"vitamin_e\",\n",
    "               \"tocopherol_alpha\",\"vitamin_k\",\"calcium\",\"copper\",\"irom\",\"magnesium\",\n",
    "               \"manganese\",\"phosphorous\",\"potassium\",\"selenium\",\"zink\",\"protein\",\n",
    "               \"alanine\",\"arginine\",\"aspartic_acid\",\"cystine\",\"glutamic_acid\",\"glycine\",\n",
    "               \"histidine\",\"hydroxyproline\",\"isoleucine\",\"leucine\",\"lysine\",\"methionine\",\n",
    "               \"phenylalanine\",\"proline\",\"serine\",\"threonine\",\"tryptophan\",\"tyrosine\",\n",
    "               \"valine\",\"carbohydrate\",\"fiber\",\"sugars\",\"fructose\",\"galactose\",\"glucose\",\n",
    "               \"lactose\",\"maltose\",\"sucrose\",\"fat\",\"saturated_fatty_acids\",\n",
    "               \"monounsaturated_fatty_acids\",\"polyunsaturated_fatty_acids\",\"fatty_acids_total_trans\",\n",
    "               \"alcohol\",\"ash\",\"caffeine\",\"theobromine\",\"water\"]\n",
    "col_by_type = get_column_types(NVCF, column_list_NVCF)\n",
    "print(col_by_type)\n",
    "\n",
    "for column in column_list_NVCF:\n",
    "    NVCF[column] = NVCF[column].apply(to_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963b21c",
   "metadata": {},
   "source": [
    "# Match columns between OFF and NVCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "NVCF_column_names_dirty = [x for x in list(NVCF.columns)]\n",
    "openfoodfacts_column_names_dirty = [x for x in list(openfoodfacts_no_nan.columns)] \n",
    "\n",
    "def clear_func(x):\n",
    "    return x.replace(\"_\",\" \").replace(\"-\",\" \").replace(\"100g\", \"\").replace(\"en\", \"\").replace(\"  \",\" \")\n",
    "\n",
    "NVCF_column_names = [clear_func(x) for x in list(NVCF.columns)]\n",
    "openfoodfacts_column_names = [clear_func(x) for x in list(openfoodfacts_no_nan.columns)] \n",
    "\n",
    "\n",
    "col_matches = match_column_names(NVCF_column_names, openfoodfacts_column_names, sim_measure=0.4)\n",
    "\n",
    "matching_cols = match_dirty(col_matches, NVCF_column_names_dirty, openfoodfacts_column_names_dirty, clear_func, clear_func)\n",
    "\n",
    "print(matching_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a189f18",
   "metadata": {},
   "source": [
    "# Change columns in OpenFoodFacts to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFF_cols_by_type = {\"g\":[]}\n",
    "for (columnNVCF, columnOFF) in matching_cols:\n",
    "    openfoodfacts_cleaner_dates[columnOFF] = openfoodfacts_cleaner_dates[columnOFF].apply(to_grams)\n",
    "    OFF_cols_by_type[\"g\"].append(columnOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77838de",
   "metadata": {},
   "source": [
    "# Match rows between OFF and NVCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cf2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_1 = NVCF['name'].to_frame()\n",
    "df_s_2 = openfoodfacts_cleaner_dates[\"food_with_ingredients\"].to_frame()\n",
    "\n",
    "names_only = True\n",
    "hard_matches = True\n",
    "\n",
    "matches = find_matches(df_s_1, df_s_2, names_only=names_only, hard_matches=hard_matches, check_duplicates=True)\n",
    "print(\"Names only, soft matches:\")\n",
    "df1_self_matches, df2_self_matches, df1_to_df2 = match_types(matches)\n",
    "show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'df1_self_matches{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df1_self_matches, f)\n",
    "with open(f'df2_self_matches{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df2_self_matches, f)\n",
    "with open(f'df1_to_df2{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df1_to_df2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_only = True\n",
    "hard_matches = False\n",
    "\n",
    "with open(f'df1_self_matches{names_only}{hard_matches}.pickle', 'rb') as f:\n",
    "    df1_self_matches = pickle.load(f)\n",
    "with open(f'df2_self_matches{names_only}{hard_matches}.pickle', 'rb') as f:\n",
    "    df2_self_matches = pickle.load(f)\n",
    "with open(f'df1_to_df2{names_only}{hard_matches}.pickle', 'rb') as f:\n",
    "    df1_to_df2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1_self_matches), df2_self_matches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ab6661",
   "metadata": {},
   "source": [
    "# Calculate difference between matched rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfoodfacts_cleaner_dates.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26258e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "good_matches = bad_matches_to_good_matches(df2_self_matches[:7000], openfoodfacts_cleaner_dates, None, 'food_with_ingredients', None)\n",
    "#print(good_matches)\n",
    "\n",
    "#(\"calories\",\"calories\")\n",
    "\n",
    "numeric_cols = []\n",
    "a = []\n",
    "for x in matching_cols:\n",
    "     a.append((x[1],x[1]))\n",
    "        \n",
    "numeric_cols = a\n",
    "\n",
    "\n",
    "comparisons = compare_matches(good_matches, numeric_cols)\n",
    "\n",
    "for i, ((ml, mr), values) in enumerate(zip(df2_self_matches, comparisons)):\n",
    "    print(\"For a match between: \")\n",
    "    print(ml.copy(deep=True).reset_index(drop=True).iloc[0,0])\n",
    "    print(\"and\")\n",
    "    print(mr.copy(deep=True).reset_index(drop=True).iloc[0,0])\n",
    "    print(\"Difference in columns: \")\n",
    "    for key, value in values.items():\n",
    "        print(f\"  {key} : {value}\")\n",
    "    print(\"_____________________________________________\")\n",
    "    if i > 200:\n",
    "        break\n",
    "\n",
    "    \n",
    "comparison_df = pd.DataFrame.from_dict(comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d8f2b",
   "metadata": {},
   "source": [
    "# Number of records comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59258e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(NVCF.index), len(openfoodfacts.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5325c5b",
   "metadata": {},
   "source": [
    "# Number of columns comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(NVCF.columns), len(openfoodfacts.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/51070985/find-out-the-percentage-of-missing-values-in-each-column-in-the-given-dataset\n",
    "def missing_values_table(df):\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mz_table = mz_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Missing Values'})\n",
    "    mz_table['% of Missing Values'] = mz_table['% of Missing Values'].round(1)\n",
    "    mz_table['Data Type'] = df.dtypes\n",
    "    mz_table['% No Missing Values'] = 100 - mz_table['% of Missing Values']\n",
    "    num_cols_missing_data = mz_table[mz_table['Missing Values'] > 0].shape[0]\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "        \"There are \" + str(num_cols_missing_data) +\n",
    "          \" columns that have missing values.\")\n",
    "    return mz_table\n",
    "\n",
    "def get_columns_by_missing_values(dff):\n",
    "    res = missing_values_table(dff)\n",
    "    res.drop(columns=[\"Missing Values\", \"Data Type\"], inplace=True)\n",
    "    return res\n",
    "\n",
    "def print_dataframe_statistics(dff):\n",
    "    mvc = get_columns_by_missing_values(dff)\n",
    "    def get_len_less_thresh(thr):\n",
    "        return len(mvc[mvc[\"% of Missing Values\"]<thr])\n",
    "    def get_len_more_thresh(thr):\n",
    "        return len(mvc[mvc[\"% of Missing Values\"]>thr])\n",
    "\n",
    "    print(f\"Number of columns with <1% missing values: {get_len_less_thresh(1)}\")\n",
    "    print(f\"Number of columns with <5% missing values: {get_len_less_thresh(5)}\")\n",
    "    print(f\"Number of columns with <10% missing values: {get_len_less_thresh(10)}\")\n",
    "    print(f\"Number of columns with <20% missing values: {get_len_less_thresh(20)}\")\n",
    "    print(f\"Number of columns with <30% missing values: {get_len_less_thresh(30)}\")\n",
    "    print(f\"Number of columns with <50% missing values: {get_len_less_thresh(50)}\")\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(f\"Number of columns with >80% missing values: {get_len_more_thresh(80)}\")\n",
    "    print(f\"Number of columns with >95% missing values: {get_len_more_thresh(95)}\")\n",
    "    print(f\"Number of columns with >99% missing values: {get_len_more_thresh(99)}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "print(\"For OpenFoodFacts\")\n",
    "print_dataframe_statistics(openfoodfacts)\n",
    "print(\"For NVCF\")\n",
    "print_dataframe_statistics(NVCF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfoodfacts_cleaner_dates[\"last_modified_datetime\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f0bfe",
   "metadata": {},
   "source": [
    "# Working with Food101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_names = [f.replace(\"_\", \" \") for f in listdir(\"/mnt/DATA/Courses/Thesis/metric calculation/food101/images/\")]\n",
    "\n",
    "records = [\"/mnt/DATA/Courses/Thesis/metric calculation/food101/images/\"+ dd +f for dd in listdir(\"/mnt/DATA/Courses/Thesis/metric calculation/food101/images/\") for f in listdir(\"/mnt/DATA/Courses/Thesis/metric calculation/food101/images/\"+dd) ]\n",
    "\n",
    "print(\"Number of food names:\", len(food_names))\n",
    "print(\"Number of records:\", len(records))\n",
    "print(\"Food names:\", food_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66e4ec",
   "metadata": {},
   "source": [
    "# Match Food101 to OpenFoodFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_1 = pd.DataFrame({\"food_name\":food_names})\n",
    "df_s_2 = openfoodfacts_cleaner_dates[\"product_name\"].to_frame()\n",
    "\n",
    "names_only = True\n",
    "hard_matches = False\n",
    "\n",
    "matches = find_matches(df_s_1, df_s_2, sim_measure= 0.2, names_only=names_only, hard_matches=hard_matches, check_duplicates=True)\n",
    "print(\"Names only, soft matches:\")\n",
    "df1_self_matches, df2_self_matches, df1_to_df2 = match_types(matches)\n",
    "show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(f'food101_self_matches{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df1_self_matches, f)\n",
    "with open(f'food101_to_df2{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df1_to_df2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6bf9f",
   "metadata": {},
   "source": [
    "# Measure image parameters for Food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937024d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = \"/mnt/DATA/Courses/Thesis/metric calculation/food101/images/\"\n",
    "measure_over_images(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec1302",
   "metadata": {},
   "source": [
    "# Load MAFood121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "MAFood121_DATA_PATH = \"/mnt/DATA/Courses/Thesis/metric calculation/Mafood121/images/\"\n",
    "MAFood121_food_names = [f.replace(\"_\", \" \") for f in listdir(MAFood121_DATA_PATH)]\n",
    "\n",
    "MAFood121_records = [MAFood121_DATA_PATH+ dd +f for dd in listdir(MAFood121_DATA_PATH) for f in listdir(MAFood121_DATA_PATH+dd) ]\n",
    "\n",
    "print(\"Number of food names:\", len(MAFood121_food_names))\n",
    "print(\"Number of records:\", len(MAFood121_records))\n",
    "print(\"Food names:\", MAFood121_food_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305197ff",
   "metadata": {},
   "source": [
    "# Match MAFood121 to OpenFoodFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eea3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_1 = pd.DataFrame({\"food_name\":MAFood121_food_names})\n",
    "df_s_2 = openfoodfacts_cleaner_dates[\"product_name\"].to_frame()\n",
    "\n",
    "names_only = True\n",
    "hard_matches = False\n",
    "\n",
    "matches = find_matches(df_s_1, df_s_2, sim_measure= 0.1, names_only=names_only, hard_matches=hard_matches, check_duplicates=True)\n",
    "print(\"Names only, soft matches:\")\n",
    "df1_self_matches, df2_self_matches, df1_to_df2 = match_types(matches)\n",
    "show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(f'MAFood121_self_matches{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df1_self_matches, f)\n",
    "with open(f'MAFood121_to_df2{names_only}{hard_matches}.pickle', 'wb') as f:\n",
    "    pickle.dump(df1_to_df2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2d925",
   "metadata": {},
   "source": [
    "# Match MAFood121 to Food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea140f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_1 = pd.DataFrame({\"food_name\":MAFood121_food_names})\n",
    "df_s_2 = pd.DataFrame({\"food_name\":food_names})\n",
    "\n",
    "names_only = True\n",
    "hard_matches = True\n",
    "\n",
    "matches = find_matches(df_s_1, df_s_2, names_only=names_only, hard_matches=hard_matches, check_duplicates=True)\n",
    "print(\"Names only, soft matches:\")\n",
    "df1_self_matches, df2_self_matches, df1_to_df2 = match_types(matches)\n",
    "show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2)\n",
    "\n",
    "print(df1_to_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e63d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58423285",
   "metadata": {},
   "source": [
    "# Measure Image parameters for MAFood121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = \"/mnt/DATA/Courses/Thesis/metric calculation/Mafood121/images/\"\n",
    "measure_over_images(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296bfa8",
   "metadata": {},
   "source": [
    "# Load Food Ingredients and Recipes Dataset with Images From Epicurious Website dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRDI_Epicurious = pd.read_csv('/mnt/DATA/Courses/Thesis/metric calculation/Food Ingredients and Recipes Dataset with Images From Epicurious Website/Food Ingredients and Recipe Dataset with Image Name Mapping.csv')\n",
    "\n",
    "FIRDI_Epicurious[\"food_with_ingredients\"] = FIRDI_Epicurious[[\"Title\", \"Cleaned_Ingredients\"]].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "FIRDI_Epicurious.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b929360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(FIRDI_Epicurious.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For FIRDI_Epicurious\")\n",
    "print_dataframe_statistics(FIRDI_Epicurious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b747e",
   "metadata": {},
   "source": [
    "# Match FIRDI_Epicurious to OpenFoodFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s_1 = FIRDI_Epicurious[\"food_with_ingredients\"].to_frame()\n",
    "df_s_2 = openfoodfacts_cleaner_dates[\"product_name\"].to_frame()\n",
    "\n",
    "names_only = False\n",
    "hard_matches = False\n",
    "\n",
    "matches = find_matches(df_s_1, df_s_2, sim_measure= 0.05, names_only=names_only, hard_matches=hard_matches, check_duplicates=True)\n",
    "print(\"Ingredients, soft matches:\")\n",
    "df1_self_matches, df2_self_matches, df1_to_df2 = match_types(matches)\n",
    "show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19992c4",
   "metadata": {},
   "source": [
    "# Image parameters for FIRDI_Epicurious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/mnt/DATA/Courses/Thesis/metric calculation/Food Ingredients and Recipes Dataset with Images From Epicurious Website/Food Images/\"\n",
    "measure_over_images(img_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3a7e9",
   "metadata": {},
   "source": [
    "# Sample OFF vs OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba71e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "off_sample = openfoodfacts_cleaner_dates[\"food_with_ingredients\"][:80000].sample(n=20000, random_state=1).to_frame()\n",
    "\n",
    "\n",
    "prev_sample = \"\"\n",
    "\n",
    "def screw_over_sample(x, augment=0.05):\n",
    "    try:\n",
    "        a = screw_over_sample.prev_sample \n",
    "    except Exception as e:\n",
    "        screw_over_sample.prev_sample  = \"\"\n",
    "    x = str(x)\n",
    "    xs = x.split(\" \")\n",
    "    sample_limit = max(1, min(int(len(xs)*(1-augment))+1, len(xs)))\n",
    "    xs = random.sample(xs, sample_limit)\n",
    "    xs += screw_over_sample.prev_sample\n",
    "    screw_over_sample.prev_sample = random.sample(xs, max(1, min(int(len(xs)*augment)+1, len(xs))))\n",
    "    return \" \".join(xs)\n",
    "\n",
    "off_sample[\"product_name\"] = off_sample[\"food_with_ingredients\"].apply(screw_over_sample)\n",
    "df_s_2 = openfoodfacts_cleaner_dates[\"food_with_ingredients\"].to_frame()\n",
    "\n",
    "names_only = True\n",
    "hard_matches = False\n",
    "\n",
    "matches = find_matches(off_sample, df_s_2, names_only=names_only, hard_matches=hard_matches, check_duplicates=True)\n",
    "print(f\"Ingredients, matches (is_hard: {hard_matches}):\")\n",
    "df1_self_matches, df2_self_matches, df1_to_df2 = match_types(matches)\n",
    "show_number_of_matches(df1_self_matches, df2_self_matches, df1_to_df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312a2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b92448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
